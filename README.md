# LLM Fine-Tuning Workspace

This repository contains Jupyter notebooks demonstrating the basics of quantization and fine-tuning large language models (LLMs) using the Unsloth library.

## Contents

- **1_quantization_basics.ipynb**: Introduction to quantization techniques for LLMs, including practical code examples and explanations.
- **2_unsloth_finetuning.ipynb**: Step-by-step guide to fine-tuning LLMs using the Unsloth library, covering data preparation, training, and evaluation.

## Getting Started

1. Clone this repository.
2. Install required Python packages (see notebook cells for details).
3. Open the notebooks in Jupyter or VS Code and follow the instructions.

## Requirements
- Python 3.8+
- Jupyter Notebook or VS Code
- Recommended: CUDA-enabled GPU for faster training


## Notebooks Overview

### Quantization Basics
- Explains quantization concepts
- Shows how to apply quantization to LLMs
- Includes code for experimenting with quantization

### Unsloth Fine-Tuning
- Demonstrates fine-tuning of large language models using the Unsloth library
- Includes instructions and code for setting up the Unsloth environment
- Uses Hugging Face datasets for loading and preparing training data
- Shows steps for loading and preparing datasets for training
- Provides example code for initializing and configuring an LLM model
- Contains training loop and evaluation procedures
- Offers tips for optimizing fine-tuning performance


## Alternative Tools for Fine-Tuning

If you do not have a suitable local environment for fine-tuning large language models, you can use cloud-based platforms such as:

- **Google Colab**: Free and paid options for running notebooks with GPU support. Simply upload the provided notebooks and run them in Colab for efficient fine-tuning.
- **Kaggle Notebooks**: Offers free GPU resources for running Jupyter notebooks online.
- **Paperspace Gradient**: Cloud platform for running machine learning notebooks with GPU access.

These platforms are ideal for users who lack local hardware (such as a CUDA-enabled GPU) or want to experiment with fine-tuning in the cloud.

## License
This project is licensed under the MIT License.

## Author
Created by linoymalakkaran.
